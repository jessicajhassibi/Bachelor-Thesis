{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a5d056",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded spacy language model: de_core_news_sm\n",
      "loaded spacy language model: en_core_web_sm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data import get_documents_list, get_cleaned_dataframe_with_topics, get_cleaned_dataframe, get_data_and_labels_lists\n",
    "from models import classification, get_embedding_model\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a57be6",
   "metadata": {},
   "source": [
    "For monolingual execution, please change WIKI_LANGUAGES in config.ini file to language code of language you want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f3933",
   "metadata": {},
   "source": [
    "### Choose Parameters: \n",
    "- method = \"muse\", \"fastText\", or \"Word2Vec\"\n",
    "    -> embedding model \n",
    "- text_type = \"cleaned_texts\", \"cleaned_texts_topics\", ... (see more in embedding_models.py)\n",
    "    -> documents the model was trained on and to do classification on\n",
    "- training_type = \"pretrained_aligned\" or \"from_scratch\"\n",
    "    -> type of training which was used\n",
    "    \n",
    "e.g. use method = \"muse\", text_type='cleaned_texts', training_type = \"pretrained_aligned\" for pretrained aligned models downloaded from facebookresearch MUSE website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5c25e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_type = \"cleaned_paragraphs\"\n",
    "method = \"muse\"\n",
    "training_type = \"pretrained_aligned\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4ff11",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "load the preprocessed texts in defined languages into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "244d06eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Abel, Ehrlich, 3., September, 1915, Cranz, Os...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ ehrlich, aufwachsen, deutsch, jüdisch, Famil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Alexander, Zemlinsky, Pseudonym, Al, Roberts,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ Zemlinskys, Großvater, Anton, Semlinsky, sta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>[ Kempff, bear,  , Jüterbog, Brandenburg, 1895...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>[Meisel, 17, September, 1897, 29, April, 1967,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>[ 1930, Prosecutor, Hallers, 1930, love, Ring,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>[Willi, Domgraf, Fassbaender, 19, February, 18...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>[ Domgraf, Fassbaender, study, first, Berlin, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1130 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   data  label\n",
       "0     [Abel, Ehrlich, 3., September, 1915, Cranz, Os...      0\n",
       "1     [ ehrlich, aufwachsen, deutsch, jüdisch, Famil...      0\n",
       "2     [Alexander, Zemlinsky, Pseudonym, Al, Roberts,...      0\n",
       "3                                                   [ ]      0\n",
       "4     [ Zemlinskys, Großvater, Anton, Semlinsky, sta...      0\n",
       "...                                                 ...    ...\n",
       "1125  [ Kempff, bear,  , Jüterbog, Brandenburg, 1895...      1\n",
       "1126  [Meisel, 17, September, 1897, 29, April, 1967,...      1\n",
       "1127  [ 1930, Prosecutor, Hallers, 1930, love, Ring,...      1\n",
       "1128  [Willi, Domgraf, Fassbaender, 19, February, 18...      1\n",
       "1129  [ Domgraf, Fassbaender, study, first, Berlin, ...      1\n",
       "\n",
       "[1130 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = list()\n",
    "if text_type == \"cleaned_texts\":\n",
    "    # case for texts\n",
    "    #df = get_cleaned_dataframe_with_topics() #todo:fix\n",
    "    cleaned_df = get_cleaned_dataframe()\n",
    "    df = cleaned_df[['data', 'label']].copy()\n",
    "    docs = get_documents_list('cleaned_texts')\n",
    "    \n",
    "elif text_type == \"cleaned_sentences\" or text_type == \"cleaned_paragraphs\":\n",
    "    # case for sentences and paragraphs\n",
    "    docs, targets = get_data_and_labels_lists(text_type)\n",
    "    df = pd.DataFrame(list(zip(docs, targets)),\n",
    "                   columns =['data', 'label'])\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd7a4c",
   "metadata": {},
   "source": [
    "Decide what types of documents you want to train your embedding model on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0454bf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba5fc05",
   "metadata": {},
   "source": [
    "NOTE FOR TRAINING NEW MODEL: \n",
    "    \n",
    "For multilingual fastText Model first train monolingual fastText models (use parameter method=\"fastText\"). \n",
    "Then use muse library to align the embeddings: https://github.com/facebookresearch/MUSE\n",
    "The command could look like this (supervised): \n",
    "``` shell\n",
    "python supervised.py --src_lang en --tgt_lang de --src_emb /Users/jessicahassibi/Bachelor-Thesis/models/classification/FastText/FastText_en_cleaned_texts.vec --tgt_emb /Users/jessicahassibi/Bachelor-Thesis/models/classification/FastText/FastText_de_cleaned_texts.vec --n_refinement 5 --dico_train default --cuda=False\n",
    "```\n",
    "or like this (unsupervised):\n",
    "``` shell\n",
    "python unsupervised.py --src_lang en --tgt_lang de --src_emb /Users/jessicahassibi/Bachelor-Thesis/models/classification/FastText/FastText_en_cleaned_texts.vec --tgt_emb /Users/jessicahassibi/Bachelor-Thesis/models/classification/FastText/FastText_de_cleaned_texts.vec --n_refinement 5 --cuda=False\n",
    "```\n",
    "\n",
    "Place the aligned vectors to /models/classification/FastText/aligned_embeddings and use code in next cell with method=\"muse\" to combine those embeddings and get multilingual word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33c5c54b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model, word_vectors = get_embedding_model(docs, text_type = text_type, training_type = training_type, method = method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bea68364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57677037"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity('composer', 'komponist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78fe7be",
   "metadata": {},
   "source": [
    "## Split data into train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76447f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['data'], df['label'] , test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e0697",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Text Classification on manually trained Embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dabf17",
   "metadata": {},
   "source": [
    "Highly Imbalanced Dataset!\n",
    "Ideas:\n",
    "- Undersampling: reduce majority class to make it equal to minority class\n",
    "- Oversampling: increase minority class to make it equal to majority class through resampling\n",
    "- K-fold cross validation\n",
    "\n",
    "### Choose Parameter: \n",
    "- resampling = False, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "781c154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampling = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c176b21d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create features from text for machine learning model.\n",
    "Steps:\n",
    "- Turn words in texts into word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "698fee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = classification.text2vec(X_train, word_vectors)\n",
    "X_test_vect = classification.text2vec(X_test, word_vectors)\n",
    "\n",
    "#- Method 1: averaging word vectors for each text\n",
    "X_train_vect_avg = classification.average_text_vector(X_train_vect)\n",
    "X_test_vect_avg = classification.average_text_vector(X_test_vect)\n",
    "\n",
    "#- Method 2: max word vectors for each text\n",
    "#- Method 3: min word vectors for each text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a657c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Intuition for Classification reports:\n",
    "- Recall = how many of this class where found over whole num of elements of this class\n",
    "- Precision = how many correctly classified among that class\n",
    "- Accuracy = num of correct predictions/ num of total predictions\n",
    "- f1-score: harmonic mean between precision and recall\n",
    "- support: num of occurrence of given class in dataset\n",
    "\n",
    "- recall macro avg = (recall_class_1 + recall_class_0) / 2\n",
    "- recall weighted avg = (support_class_0 * recall_class_0 + support_class_1 * recall_class_1) / (support_class_0 + support_class_1)\n",
    "- scikit learn classification_report average parameter explanation:\n",
    "\n",
    "-average=micro: compute f1 by considering total true positives, false negatives and false positives (no matter of the prediction for each label in the dataset) (= Accuracy?)\n",
    "\n",
    "-average=macro: compute f1 for each label, and returns the average without considering the proportion for each label in the dataset. (treats all classes the same)\n",
    "\n",
    "-average=weighted: compute f1 for each label, and returns the average considering the proportion for each label in the dataset.\n",
    "\n",
    "-average=samples: compute f1 for each instance, and returns the average. Use it for multilabel classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05687a95",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Instantiate and fit a classificator model on top of the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a76f007",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION WITH:OVERSAMPLING\n",
      "0    750\n",
      "1    750\n",
      "Name: label, dtype: int64\n",
      "############################################################################\n",
      "Processing Multinomial Naive Bayes Classification\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    verfolgt       0.86      0.86      0.86       186\n",
      "    begnadet       0.35      0.35      0.35        40\n",
      "\n",
      "    accuracy                           0.77       226\n",
      "   macro avg       0.61      0.61      0.61       226\n",
      "weighted avg       0.77      0.77      0.77       226\n",
      "\n",
      "classes in y_pred: {0, 1} classes in y_test: {0, 1}\n",
      "############################################################################\n",
      "############################################################################\n",
      "Processing SVM Classification\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    verfolgt       0.81      0.91      0.86       166\n",
      "    begnadet       0.62      0.42      0.50        60\n",
      "\n",
      "    accuracy                           0.78       226\n",
      "   macro avg       0.72      0.66      0.68       226\n",
      "weighted avg       0.76      0.78      0.76       226\n",
      "\n",
      "classes in y_pred: {0, 1} classes in y_test: {0, 1}\n",
      "############################################################################\n",
      "CLASSIFICATION WITH:UNDERSAMPLING\n",
      "0    154\n",
      "1    154\n",
      "Name: label, dtype: int64\n",
      "############################################################################\n",
      "Processing Multinomial Naive Bayes Classification\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    verfolgt       0.78      0.85      0.81       170\n",
      "    begnadet       0.38      0.27      0.31        56\n",
      "\n",
      "    accuracy                           0.71       226\n",
      "   macro avg       0.58      0.56      0.56       226\n",
      "weighted avg       0.68      0.71      0.69       226\n",
      "\n",
      "classes in y_pred: {0, 1} classes in y_test: {0, 1}\n",
      "############################################################################\n",
      "############################################################################\n",
      "Processing SVM Classification\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    verfolgt       0.69      0.88      0.77       147\n",
      "    begnadet       0.55      0.28      0.37        79\n",
      "\n",
      "    accuracy                           0.67       226\n",
      "   macro avg       0.62      0.58      0.57       226\n",
      "weighted avg       0.64      0.67      0.63       226\n",
      "\n",
      "classes in y_pred: {0, 1} classes in y_test: {0, 1}\n",
      "############################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not resampling:\n",
    "    print(\"CLASSIFICATION WITHOUT SAMPLING:\")\n",
    "    #y_pred = classification.classify_predict(X_train_vect_avg, X_test_vect_avg, y_train, \"Random Forest\")\n",
    "    #classification.print_classification_report(y_test, y_pred)\n",
    "\n",
    "    y_pred = classification.classify_predict(X_train_vect_avg, X_test_vect_avg, y_train, \"Multinomial Naive Bayes\")\n",
    "    classification.print_classification_report(y_test, y_pred)\n",
    "\n",
    "    y_pred = classification.classify_predict(X_train_vect_avg, X_test_vect_avg, y_train, \"SVM\")\n",
    "    classification.print_classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(y_train.value_counts())\n",
    "    y_train.value_counts().plot.pie(autopct='%0.2f')\n",
    "    \n",
    "else:\n",
    "    ros = RandomOverSampler(sampling_strategy=\"not majority\")\n",
    "    rus = RandomUnderSampler(random_state=0, sampling_strategy=\"majority\")\n",
    "    \n",
    "    for sampler, method in [(ros, \"OVERSAMPLING\"),(rus, \"UNDERSAMPLING\")]:\n",
    "        print(f\"CLASSIFICATION WITH:{method}\")\n",
    "            \n",
    "        X_train_2d = X_train.values.reshape(-1, 1) # puts each elem of array in own array\n",
    "        X_res, y_res = sampler.fit_resample(X_train_2d, y_train)\n",
    "        \n",
    "        print(y_res.value_counts())\n",
    "        y_res.value_counts().plot.pie(autopct='%0.2f')\n",
    "        \n",
    "        #X_res revert 2_dimensionality\n",
    "        X_res_1d = X_res.flatten()\n",
    "\n",
    "        # classification with resampled vectors\n",
    "        X_res_vect = classification.text2vec(X_res_1d, word_vectors)\n",
    "        X_res_vect_avg = classification.average_text_vector(X_res_vect)\n",
    "\n",
    "        #y_res_pred = classification.classify_predict(X_res_vect_avg, X_test_vect_avg, y_res, \"Random Forest\")\n",
    "        #classification.print_classification_report(y_test, y_res_pred)\n",
    "        y_res_pred = classification.classify_predict(X_res_vect_avg, X_test_vect_avg, y_res, \"Multinomial Naive Bayes\")\n",
    "        classification.print_classification_report(y_test, y_res_pred)\n",
    "        y_res_pred = classification.classify_predict(X_res_vect_avg, X_test_vect_avg, y_res, \"SVM\")\n",
    "        classification.print_classification_report(y_test, y_res_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e69b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis] *",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
